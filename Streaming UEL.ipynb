{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D, get_test_data\n",
    "from matplotlib import cm\n",
    "from time import time\n",
    "from itertools import permutations, product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normP = lambda x: np.maximum(x, 1e-9)/np.sum(np.maximum(x, 1e-9))\n",
    "normc = lambda x: x/np.linalg.norm(x)\n",
    "str_arr = lambda arr: \" \".join(\"%.2f\"%x for x in arr)\n",
    "str_mat = lambda mat: \"\\n\".join(str_arr(arr) for arr in mat) \n",
    "def normRowP(A, dim = 0):\n",
    "    #norm P across dim: 0: cols, 1: rows\n",
    "    r,c = A.shape\n",
    "    if dim == 0: \n",
    "        rep = (r,1)\n",
    "    elif dim == 1:\n",
    "        rep = (1,c)\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    arr = np.sum(A, axis = dim)\n",
    "    A_new = np.divide(A, np.tile(arr, (2,1)))\n",
    "    \n",
    "def genConfMat(k, alpha):\n",
    "    # k is the size of the confusion matrix\n",
    "    # alpha is the lower limit for the diagonal entry\n",
    "    C = alpha*np.eye(k) + (1-alpha)*np.array([normP(1 + np.random.rand(k)) for i in range(k)])\n",
    "    return C\n",
    "def tensorProdk(a,k):\n",
    "    res = a\n",
    "    for _ in range(k-1):\n",
    "        res = np.tensordot(res, a, axes = 0)\n",
    "    return res\n",
    "\n",
    "def tensorwhiten(T,W):\n",
    "    # T(W,W,W)\n",
    "    I,J,K = T.shape\n",
    "    T_new = np.zeros((I,J,K))\n",
    "    for i,j,k in product(range(I), range(J), range(K)):\n",
    "        T_new[i,j,k] = np.sum([ T[i_p,j_p,k_p]*W[i_p,i]*W[j_p,j]*W[k_p,k] for i_p,j_p,k_p in product(range(I), range(J), range(K))])\n",
    "    return T_new\n",
    "\n",
    "def tensormult2(T, v):\n",
    "    # T(I,v,v)\n",
    "    I,J,K = T.shape\n",
    "    v_new = np.zeros(I)\n",
    "    for i in range(I):\n",
    "        v_new[i] = np.sum([T[i,j_p,k_p]*v[j_p]*v[k_p] for j_p,k_p in product(range(J), range(K))])\n",
    "    return v_new\n",
    "\n",
    "def tensormult3(T, v):\n",
    "    # T(v,v,v)\n",
    "    I,J,K = T.shape\n",
    "    val = np.sum([T[i_p,j_p,k_p]*v[i_p]*v[j_p]*v[k_p] for i_p,j_p,k_p in product(range(I), range(J), range(K))])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Ensemble Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifierEnsemble:   \n",
    "    def __init__(self, params):\n",
    "        self.ph_true = params['ph']\n",
    "        self.num_classifier = params['num_classifier']\n",
    "        self.conf_true = params['conf'] # dictionary of confusion matrices\n",
    "        # each row: one true class \n",
    "        self.num_class = len(self.ph_true) #number of classes\n",
    "        self.ph_est = np.zeros(self.num_class) \n",
    "        self.conf_est = {i: np.zeros((self.num_class,self.num_class)) for i in range(self.num_classifier)}\n",
    "    \n",
    "    def genData(self, n, adv_map, adv_p, seed = None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        # fraction of adversarial examples\n",
    "        adv_p = min(1, max(0, adv_p))\n",
    "        # adversarial examples\n",
    "        adv_arr = np.random.choice([False, True], p = [1-adv_p, adv_p], size = n)\n",
    "        # true labels\n",
    "        h_arr = np.random.choice(range(self.num_class), p = self.ph_true, size = n)\n",
    "        # labels from the classifiers\n",
    "        labels_arr = np.zeros((n,self.num_classifier, self.num_class))\n",
    "        for i in range(n):\n",
    "            h = h_arr[i]\n",
    "            if adv_arr[i]: #adversarial Example\n",
    "                labels_arr[i,:,adv_map[h]] = 1\n",
    "            else:\n",
    "                for j in range(self.num_classifier):\n",
    "                    l = np.random.choice(range(self.num_class), p = self.conf_true[j][h,:])\n",
    "                    labels_arr[i,j,l] = 1\n",
    "        return adv_arr, h_arr, labels_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate ensemble of classifiers (Start  of the Experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:0.30 0.15 0.15 0.15 0.15 0.15 0.10 0.10 0.30 0.15 0.30 0.30 0.15 0.15 0.10 0.30 0.30 0.10 0.10 0.30\n"
     ]
    }
   ],
   "source": [
    "num_classifier = 20\n",
    "num_class = 10\n",
    "ph_true = np.random.rand(num_class); ph_true = ph_true/np.sum(ph_true);\n",
    "alpha_arr = np.minimum(1, np.array([0.2, 0.05, 0.0])+1.0/num_class) #minimum accuracy per worker\n",
    "alpha_p = [0.5, 0.3, 0.2] # probability of choosing alpha from alpha_arr\n",
    "alpha = np.random.choice(alpha_arr, p = alpha_p, size = num_classifier)\n",
    "print \"Alpha:%s\"%str_arr(alpha)\n",
    "conf_dict = {i: genConfMat(num_class, alpha[i]) for i in range(num_classifier)}\n",
    "E = classifierEnsemble({'ph': ph_true, 'num_classifier': num_classifier, 'conf': conf_dict})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition classifiers in three groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = {}\n",
    "while len(g.keys()) < 3:\n",
    "    groups = np.random.choice([0,1,2], p = [1/3.0, 1/3.0, 1/3.0], size = num_classifier)\n",
    "    g ={i: list(groups ==i) for i in range(3) if np.sum(groups ==i) > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing true parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_true = {j: np.mean([conf_dict[i] for i,f in enumerate(g[j]) if f],axis = 0) for j in range(3)}\n",
    "M2_true = {}; M3_true = {}\n",
    "for j in range(3):\n",
    "    M = np.zeros((num_class, num_class))\n",
    "    T = np.zeros((num_class, num_class, num_class))\n",
    "    for i in range(num_class):\n",
    "        M += ph_true[i]*tensorProdk(mu_true[j][i],2)\n",
    "        T += ph_true[i]*tensorProdk(mu_true[j][i],3)\n",
    "    M2_true[j] = M\n",
    "    M3_true[j] = T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adversarial inputs \n",
    "--------------------\n",
    "- This skews the estimation of confusion matrix.\n",
    "- The classifiers are conditionally independent on real inputs.\n",
    "'''\n",
    "# Adversarial input parameters\n",
    "#adv_map = np.random.permutation(range(num_class)) # map of adversarial labels\n",
    "adv_map = np.mod(1+np.arange(num_class), num_class) # map of adversarial labels\n",
    "adv_p = 0.1 # probability that an input is adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 20000\n",
    "## Data Format: \n",
    "# - labels[i,j,:] = one hot encoding of label for time i and classifier j \n",
    "# - true_label[i] = true label (categorical) for time i\n",
    "# - adv[i] = 'True' if adversarial ip o/w 'False'\n",
    "adv, true_label, labels = E.genData(num_data, adv_map, adv_p)\n",
    "# test set\n",
    "adv_test, true_label_test, labels_test = E.genData(num_data, adv_map, adv_p)\n",
    "# Data aggregation\n",
    "Z ={i: np.mean(labels[:,g[i],:], 1) for i in range(3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to symmetric moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "perms = [(1,2,0), (0,1,2), (2,0,1)]\n",
    "M2 = {}; M3 = {}; \n",
    "for p in perms:\n",
    "    a, b, c = p\n",
    "    Mcb = np.mean([np.tensordot(Z[c][j,:], Z[b][j,:], axes =0) for j in range(num_data)], axis = 0)\n",
    "    Mab = np.mean([np.tensordot(Z[a][j,:], Z[b][j,:], axes =0) for j in range(num_data)], axis = 0)\n",
    "    Za_prime = [np.dot(np.dot(Mcb, np.linalg.inv(Mab)), Z[a][j]) for j in range(num_data)]\n",
    "    \n",
    "    Mca = np.mean([np.tensordot(Z[c][j,:], Z[a][j,:], axes =0) for j in range(num_data)], axis = 0)\n",
    "    Mba = np.mean([np.tensordot(Z[b][j,:], Z[a][j,:], axes =0) for j in range(num_data)], axis = 0)\n",
    "    Zb_prime = [np.dot(np.dot(Mca, np.linalg.inv(Mba)), Z[b][j,:]) for j in range(num_data)]\n",
    "    \n",
    "    M2[c] = np.mean([np.tensordot(Za_prime[j], Zb_prime[j], axes =0) for j in range(num_data)], axis = 0)\n",
    "    M3[c] = np.mean([np.tensordot(np.tensordot(Za_prime[j], Zb_prime[j], axes = 0), Z[c][j,:], axes = 0) for j in range(num_data)], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical moments vs True moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error M2_true[0] vs M2_est[0]:0.03036\n",
      "Error M3_true[0] vs M3_est[0]:0.03113\n",
      "Error M2_true[1] vs M2_est[1]:0.03048\n",
      "Error M3_true[1] vs M3_est[1]:0.03286\n",
      "Error M2_true[2] vs M2_est[2]:0.03076\n",
      "Error M3_true[2] vs M3_est[2]:0.03314\n"
     ]
    }
   ],
   "source": [
    "for j in range(3):\n",
    "    print 'Error M2_true[%d] vs M2_est[%d]:%.5f'%(j,j, np.linalg.norm(M2_true[j]-M2[j]))\n",
    "    print 'Error M3_true[%d] vs M3_est[%d]:%.5f'%(j,j, np.linalg.norm(M3_true[j]-M3[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Recovering Group Confusion Matrices using Tensor Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Group:(0)**\n",
      "Recovered eigenpairs:\n",
      "6.03:[-0.17 0.05 0.08 -0.10 0.02 0.09 -0.34 -0.75 -0.47 -0.19]\n",
      "6.73:[-0.15 0.09 0.29 0.89 -0.19 -0.18 0.13 0.07 -0.07 0.03]\n",
      "8.78:[-0.10 0.02 0.08 -0.02 0.05 0.05 -0.16 -0.30 0.65 0.66]\n",
      "6.76:[-0.14 0.14 0.08 -0.07 0.22 0.76 0.56 0.05 -0.09 0.02]\n",
      "6.32:[-0.15 0.18 0.20 -0.28 0.51 -0.65 0.34 0.12 -0.14 0.08]\n",
      "5.65:[-0.17 0.03 -0.03 -0.05 0.11 -0.03 -0.21 0.19 0.57 -0.74]\n",
      "6.26:[-0.16 0.21 -0.93 0.12 -0.07 -0.08 0.11 0.04 -0.12 0.08]\n",
      "7.27:[-0.13 0.16 0.17 -0.41 -0.83 -0.11 0.20 0.10 -0.03 0.01]\n",
      "5.89:[-0.18 -0.96 -0.10 0.01 -0.07 -0.05 0.14 0.03 -0.08 0.06]\n",
      "7.41:[-0.12 0.05 0.10 -0.05 0.10 0.18 -0.59 0.64 -0.31 0.26]\n",
      "**Group:(2)**\n",
      "Recovered eigenpairs:\n",
      "7.38:[-0.13 0.01 0.02 -0.11 -0.18 0.10 -0.71 -0.65 0.02 -0.07]\n",
      "8.79:[-0.10 0.05 -0.01 -0.07 -0.03 -0.01 -0.13 0.36 -0.73 -0.55]\n",
      "6.73:[-0.15 0.23 0.41 0.76 0.20 -0.25 0.14 -0.20 0.05 -0.11]\n",
      "7.24:[-0.13 0.08 0.16 -0.24 -0.75 -0.31 0.45 -0.11 0.13 -0.07]\n",
      "5.90:[-0.18 -0.92 -0.22 0.23 0.01 -0.04 0.10 -0.03 0.05 -0.05]\n",
      "6.09:[-0.16 0.07 0.06 -0.05 0.06 -0.09 -0.36 0.64 0.63 -0.10]\n",
      "6.77:[-0.14 0.03 0.28 -0.10 0.11 0.85 0.35 -0.10 0.14 -0.12]\n",
      "6.27:[-0.16 0.38 -0.87 0.16 0.03 0.05 0.18 -0.10 0.08 -0.04]\n",
      "5.68:[-0.16 0.03 0.04 0.00 -0.08 0.10 -0.10 0.20 -0.32 0.90]\n",
      "6.32:[-0.15 0.00 0.07 -0.57 0.64 -0.41 0.22 -0.17 0.01 0.02]\n",
      "**Group:(1)**\n",
      "Recovered eigenpairs:\n",
      "6.70:[-0.15 0.12 0.22 0.89 -0.12 0.15 0.08 0.05 0.25 0.08]\n",
      "6.76:[-0.14 0.09 0.07 -0.36 0.16 0.60 0.61 0.21 0.21 0.01]\n",
      "5.63:[-0.18 0.05 0.08 0.08 -0.07 0.03 0.06 -0.07 -0.62 -0.74]\n",
      "7.25:[-0.13 0.07 0.21 -0.26 -0.68 -0.53 0.26 0.12 0.19 0.04]\n",
      "6.33:[-0.15 0.08 0.21 -0.03 0.74 -0.55 0.13 -0.15 0.15 0.06]\n",
      "6.27:[-0.16 0.33 -0.91 0.07 0.01 -0.14 0.02 0.05 0.08 0.06]\n",
      "5.88:[-0.18 -0.95 -0.21 0.04 0.02 -0.04 0.04 0.05 0.07 0.03]\n",
      "7.41:[-0.12 0.07 0.12 -0.16 0.08 0.05 -0.68 0.68 0.11 0.01]\n",
      "6.07:[-0.16 0.06 0.06 -0.21 -0.13 0.27 -0.45 -0.72 0.32 -0.05]\n",
      "8.75:[-0.09 0.04 0.07 -0.04 -0.04 0.07 -0.04 -0.07 -0.63 0.76]\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rank = num_class\n",
    "Maxiter = 50; num_init = 5;\n",
    "# initialiaztion\n",
    "W ={}; mu_est = {}; ph_est = {}\n",
    "disp = False\n",
    "for j in range(3):\n",
    "    c = perms[j][2] # group c parameters\n",
    "    print '**Group:(%d)**'%c\n",
    "    ## whitenning M2\n",
    "    [U, l, V] = np.linalg.svd(M2[j]); \n",
    "    L = np.diag(l)\n",
    "    W = np.dot( U[:, :rank], np.sqrt(np.linalg.inv(L[:rank, :rank])) );\n",
    "    M3W = tensorwhiten(M3[j], W)\n",
    "    ## tensor decomposition\n",
    "    alpha_arr = np.zeros(rank)\n",
    "    ph_est_int = np.zeros(rank)\n",
    "    v_arr = np.zeros((rank, rank))\n",
    "    mu_est_int = np.zeros((rank, rank))\n",
    "    #------\n",
    "    for i in range(rank): # loop over eigenvalues\n",
    "        for t in range(num_init): # loop over initialization\n",
    "            max_val = -1000;\n",
    "            v_old = np.random.rand(rank); v_old = v_old/np.linalg.norm(v_old);\n",
    "            for j in range(Maxiter): # power method iteration\n",
    "                v_new = tensormult2(M3W, v_old); v_new = v_new/np.linalg.norm(v_new);\n",
    "                v_old = v_new\n",
    "            new_val = tensormult3(M3W, v_new)\n",
    "            if new_val > max_val:\n",
    "                alpha_arr[i] = new_val\n",
    "                v_arr[:,i] = v_new\n",
    "                max_val = new_val\n",
    "        # Unwhittening the vectors are stacked into columns for mu_est_int\n",
    "        ph_est_int[i] = alpha_arr[i]**(-2)\n",
    "        mu_est_int[:,i] = normP(np.dot(np.linalg.inv(W.T), alpha_arr[i]*v_arr[:,i]))\n",
    "        # deflate tensor\n",
    "        M3W -= alpha_arr[i]*tensorProdk(v_arr[:,i],3)\n",
    "        if disp: print 'Tensor norm after %d-th eigenpair:%.3f'%(i, np.linalg.norm(M3W))\n",
    "    #--------\n",
    "    print 'Recovered eigenpairs:\\n%s'%\"\\n\".join('%.2f:[%s]'%(alpha_arr[i], str_arr(v_arr[:,i])) for i in range(num_class))       \n",
    "    if disp: print 'mu_est_int[%d]:\\n%s'%(c,\"\\n\".join(\"%s\"%str_arr(mu_est_int[i,:]) for i in range(rank)))\n",
    "    # finding the correct permutation of eigenpairs\n",
    "    # - the l-th col with the highest l-th row entry becomes the new l-th the col \n",
    "    ind = [np.argmax(mu_est_int[i,:]) for i in range(num_class)]\n",
    "    if disp: print 'Correct order:%s'%str_arr(ind)\n",
    "    mu_est[c] = mu_est_int[:,ind] # rearranging the rows(?)\n",
    "    ph_est[c] = ph_est_int[ind]\n",
    "    #print 'ph_est:%s'%str_arr(ph_est[c])\n",
    "    #print 'mu_est:\\n%s'%\"\\n\".join(\"%s\"%str_arr(mu_est[c][:,i]) for i in range(rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated vs True Group Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collective Confusion Matrix\n",
      "**Group:(0)**\n",
      "ph_est:0.03 0.02 0.01 0.03 0.03 0.02 0.02 0.03 0.03 0.02\n",
      "mu_est:\n",
      "0.78 0.02 0.02 0.07 0.00 0.03 0.02 0.03 0.01 0.01\n",
      "0.03 0.89 0.01 0.03 0.02 0.01 0.03 0.05 0.03 0.02\n",
      "0.02 0.01 0.84 0.04 0.02 0.02 0.00 0.04 0.00 0.02\n",
      "0.05 0.01 0.02 0.61 0.00 0.02 0.02 0.03 0.02 0.02\n",
      "0.00 0.00 0.02 0.06 0.85 0.04 0.03 0.04 0.02 0.01\n",
      "0.02 0.02 0.03 0.03 0.03 0.83 0.03 0.03 0.02 0.02\n",
      "0.03 0.00 0.01 0.07 0.01 0.03 0.82 0.04 0.01 0.03\n",
      "0.02 0.03 0.02 0.00 0.02 0.00 0.00 0.64 0.02 0.04\n",
      "0.02 0.01 0.00 0.07 0.03 0.01 0.01 0.03 0.85 0.05\n",
      "0.03 0.02 0.03 0.02 0.01 0.01 0.05 0.05 0.02 0.79\n",
      "mu_true:\n",
      "0.24 0.09 0.08 0.09 0.09 0.08 0.08 0.08 0.09 0.09\n",
      "0.09 0.23 0.08 0.09 0.08 0.09 0.08 0.09 0.09 0.09\n",
      "0.09 0.09 0.25 0.07 0.09 0.09 0.08 0.09 0.08 0.09\n",
      "0.09 0.07 0.08 0.24 0.09 0.09 0.09 0.08 0.09 0.08\n",
      "0.08 0.09 0.08 0.08 0.23 0.09 0.09 0.09 0.09 0.08\n",
      "0.09 0.08 0.08 0.09 0.09 0.24 0.08 0.08 0.08 0.09\n",
      "0.08 0.08 0.08 0.09 0.09 0.08 0.23 0.08 0.09 0.09\n",
      "0.09 0.08 0.09 0.09 0.08 0.09 0.09 0.23 0.08 0.09\n",
      "0.08 0.09 0.08 0.09 0.10 0.07 0.09 0.09 0.23 0.08\n",
      "0.08 0.09 0.08 0.08 0.09 0.09 0.08 0.09 0.09 0.23\n",
      "Error:1.87810376162\n",
      "**Group:(1)**\n",
      "ph_est:0.02 0.02 0.01 0.03 0.03 0.02 0.02 0.03 0.03 0.02\n",
      "mu_est:\n",
      "0.79 0.02 0.01 0.04 0.03 0.02 0.02 0.00 0.02 0.04\n",
      "0.02 0.87 0.02 0.06 0.03 0.01 0.02 0.02 0.01 0.02\n",
      "0.02 0.02 0.87 0.08 0.02 0.02 0.01 0.02 0.01 0.04\n",
      "0.01 0.02 0.00 0.58 0.02 0.04 0.00 0.04 0.01 0.03\n",
      "0.03 0.01 0.00 0.04 0.88 0.02 0.01 0.04 0.01 0.01\n",
      "0.04 0.01 0.01 0.07 0.00 0.83 0.02 0.00 0.03 0.00\n",
      "0.04 0.01 0.04 0.02 0.01 0.00 0.86 0.06 0.00 0.02\n",
      "0.02 0.03 0.02 0.04 0.00 0.03 0.01 0.73 0.01 0.03\n",
      "0.02 0.00 0.02 0.04 0.01 0.01 0.04 0.04 0.88 0.02\n",
      "0.02 0.01 0.00 0.03 0.02 0.01 0.00 0.04 0.02 0.79\n",
      "mu_true:\n",
      "0.25 0.08 0.08 0.07 0.09 0.07 0.09 0.10 0.09 0.08\n",
      "0.08 0.25 0.08 0.09 0.08 0.09 0.08 0.09 0.08 0.08\n",
      "0.08 0.08 0.25 0.08 0.08 0.09 0.07 0.09 0.08 0.08\n",
      "0.07 0.09 0.09 0.24 0.07 0.09 0.08 0.08 0.10 0.09\n",
      "0.09 0.08 0.08 0.10 0.25 0.08 0.09 0.09 0.06 0.08\n",
      "0.08 0.08 0.09 0.08 0.08 0.26 0.07 0.09 0.08 0.09\n",
      "0.07 0.08 0.07 0.08 0.09 0.08 0.27 0.09 0.09 0.08\n",
      "0.08 0.07 0.10 0.08 0.08 0.09 0.08 0.25 0.08 0.08\n",
      "0.08 0.08 0.09 0.07 0.10 0.08 0.08 0.09 0.26 0.07\n",
      "0.10 0.08 0.08 0.07 0.08 0.09 0.08 0.08 0.07 0.25\n",
      "Error:1.87739448191\n",
      "**Group:(2)**\n",
      "ph_est:0.02 0.02 0.01 0.03 0.03 0.02 0.02 0.03 0.03 0.02\n",
      "mu_est:\n",
      "0.82 0.01 0.03 0.02 0.03 0.01 0.00 0.06 0.02 0.02\n",
      "0.02 0.87 0.02 0.04 0.01 0.02 0.02 0.00 0.02 0.02\n",
      "0.02 0.01 0.84 0.04 0.01 0.01 0.01 0.05 0.03 0.00\n",
      "0.02 0.02 0.00 0.62 0.03 0.01 0.02 0.06 0.01 0.02\n",
      "0.02 0.01 0.01 0.04 0.86 0.00 0.04 0.03 0.02 0.02\n",
      "0.01 0.03 0.01 0.03 0.00 0.86 0.01 0.05 0.01 0.04\n",
      "0.01 0.02 0.01 0.05 0.01 0.01 0.81 0.02 0.01 0.01\n",
      "0.04 0.02 0.03 0.05 0.02 0.04 0.03 0.68 0.01 0.02\n",
      "0.02 0.00 0.04 0.05 0.00 0.01 0.03 0.04 0.87 0.00\n",
      "0.02 0.01 0.01 0.06 0.02 0.04 0.03 0.01 0.00 0.84\n",
      "mu_true:\n",
      "0.25 0.09 0.07 0.09 0.08 0.09 0.08 0.09 0.08 0.07\n",
      "0.08 0.24 0.09 0.09 0.08 0.09 0.08 0.09 0.08 0.08\n",
      "0.08 0.09 0.26 0.07 0.09 0.09 0.08 0.08 0.07 0.07\n",
      "0.08 0.09 0.09 0.25 0.09 0.09 0.07 0.08 0.09 0.08\n",
      "0.08 0.09 0.08 0.09 0.26 0.08 0.09 0.09 0.08 0.08\n",
      "0.09 0.09 0.08 0.09 0.09 0.25 0.07 0.08 0.09 0.08\n",
      "0.09 0.09 0.08 0.08 0.07 0.08 0.25 0.09 0.09 0.08\n",
      "0.07 0.09 0.08 0.08 0.09 0.08 0.09 0.25 0.09 0.09\n",
      "0.08 0.07 0.07 0.08 0.09 0.09 0.08 0.09 0.25 0.09\n",
      "0.09 0.08 0.09 0.08 0.09 0.06 0.10 0.08 0.09 0.24\n",
      "Error:1.8792754646\n",
      "**Avg ph_est:0.02 0.02 0.01 0.03 0.03 0.02 0.02 0.03 0.03 0.02**\n"
     ]
    }
   ],
   "source": [
    "print \"Collective Confusion Matrix\"\n",
    "for j in range(3):\n",
    "    print '**Group:(%d)**'%j\n",
    "    print 'ph_est:%s'%str_arr(ph_est[j])\n",
    "    print 'mu_est:\\n%s'%str_mat(mu_est[j])\n",
    "    print 'mu_true:\\n%s'%str_mat(mu_true[j])\n",
    "    print 'Error:%s'%np.linalg.norm(mu_true[j]- mu_est[j])\n",
    "ph_est_avg = np.mean([ph_est[j] for j in range(3)], axis = 0)\n",
    "print '**Avg ph_est:%s**'%str_arr(ph_est_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Individual Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Individual Confusion Matrix**\n",
      "---------------\n",
      "Classifier: 0\n",
      "---------------\n",
      "Estimate\n",
      "0.24 0.08 0.08 0.08 0.08 0.07 0.09 0.08 0.08 0.08\n",
      "0.09 0.27 0.08 0.09 0.07 0.08 0.08 0.08 0.07 0.08\n",
      "0.07 0.08 0.21 0.09 0.08 0.08 0.08 0.08 0.07 0.09\n",
      "0.08 0.08 0.08 0.19 0.08 0.09 0.09 0.09 0.08 0.08\n",
      "0.08 0.08 0.09 0.10 0.31 0.08 0.09 0.09 0.07 0.09\n",
      "0.08 0.07 0.08 0.09 0.07 0.26 0.08 0.08 0.08 0.08\n",
      "0.08 0.08 0.10 0.09 0.08 0.08 0.24 0.10 0.08 0.09\n",
      "0.09 0.09 0.10 0.10 0.08 0.09 0.10 0.22 0.09 0.09\n",
      "0.09 0.08 0.09 0.09 0.07 0.08 0.08 0.08 0.29 0.08\n",
      "0.08 0.08 0.09 0.08 0.08 0.08 0.09 0.09 0.08 0.25\n",
      "groundtruth\n",
      "0.27 0.10 0.09 0.09 0.06 0.06 0.10 0.06 0.10 0.07\n",
      "0.09 0.28 0.09 0.08 0.10 0.07 0.10 0.08 0.06 0.07\n",
      "0.06 0.08 0.27 0.10 0.11 0.09 0.07 0.07 0.07 0.09\n",
      "0.07 0.07 0.06 0.27 0.08 0.08 0.11 0.06 0.10 0.06\n",
      "0.06 0.05 0.09 0.10 0.27 0.10 0.07 0.08 0.07 0.10\n",
      "0.10 0.06 0.06 0.06 0.08 0.28 0.06 0.09 0.11 0.06\n",
      "0.09 0.07 0.11 0.07 0.09 0.07 0.26 0.10 0.08 0.11\n",
      "0.09 0.10 0.07 0.09 0.08 0.06 0.08 0.30 0.08 0.10\n",
      "0.09 0.09 0.08 0.09 0.06 0.10 0.07 0.06 0.27 0.06\n",
      "0.06 0.10 0.09 0.06 0.07 0.10 0.08 0.10 0.07 0.28\n",
      "Error:0.21\n",
      "---------------\n",
      "Classifier: 1\n",
      "---------------\n",
      "Estimate\n",
      "0.25 0.09 0.10 0.10 0.09 0.09 0.09 0.09 0.09 0.09\n",
      "0.08 0.26 0.08 0.08 0.07 0.08 0.07 0.08 0.08 0.08\n",
      "0.08 0.08 0.20 0.08 0.08 0.08 0.08 0.09 0.07 0.08\n",
      "0.08 0.08 0.09 0.19 0.08 0.08 0.08 0.09 0.08 0.08\n",
      "0.11 0.10 0.10 0.11 0.32 0.10 0.10 0.10 0.09 0.09\n",
      "0.07 0.08 0.08 0.09 0.07 0.26 0.08 0.09 0.08 0.08\n",
      "0.08 0.07 0.08 0.08 0.07 0.07 0.24 0.08 0.07 0.08\n",
      "0.09 0.09 0.10 0.09 0.08 0.09 0.09 0.22 0.08 0.09\n",
      "0.08 0.07 0.09 0.09 0.08 0.08 0.08 0.08 0.30 0.08\n",
      "0.08 0.08 0.08 0.09 0.07 0.08 0.08 0.08 0.08 0.24\n",
      "groundtruth\n",
      "0.27 0.07 0.09 0.10 0.08 0.08 0.08 0.07 0.10 0.07\n",
      "0.10 0.28 0.09 0.08 0.06 0.09 0.06 0.09 0.07 0.07\n",
      "0.08 0.07 0.28 0.09 0.09 0.07 0.09 0.09 0.07 0.08\n",
      "0.08 0.07 0.06 0.26 0.09 0.07 0.07 0.08 0.08 0.07\n",
      "0.10 0.10 0.10 0.10 0.30 0.10 0.07 0.08 0.09 0.10\n",
      "0.05 0.09 0.09 0.09 0.08 0.26 0.07 0.09 0.10 0.06\n",
      "0.10 0.07 0.06 0.07 0.08 0.07 0.29 0.07 0.05 0.10\n",
      "0.08 0.11 0.10 0.06 0.10 0.09 0.10 0.27 0.05 0.07\n",
      "0.06 0.08 0.07 0.07 0.07 0.08 0.08 0.06 0.28 0.10\n",
      "0.08 0.06 0.06 0.07 0.05 0.10 0.09 0.09 0.09 0.27\n",
      "Error:0.19\n",
      "---------------\n",
      "Classifier: 2\n",
      "---------------\n",
      "Estimate\n",
      "0.24 0.10 0.09 0.09 0.09 0.09 0.09 0.10 0.09 0.09\n",
      "0.08 0.24 0.08 0.09 0.08 0.08 0.08 0.08 0.08 0.08\n",
      "0.08 0.07 0.19 0.09 0.08 0.08 0.08 0.09 0.08 0.08\n",
      "0.08 0.08 0.08 0.16 0.08 0.08 0.07 0.07 0.07 0.08\n",
      "0.09 0.09 0.10 0.09 0.28 0.09 0.09 0.09 0.09 0.10\n",
      "0.08 0.08 0.09 0.11 0.09 0.25 0.08 0.09 0.08 0.07\n",
      "0.09 0.08 0.09 0.09 0.08 0.07 0.23 0.08 0.08 0.09\n",
      "0.08 0.09 0.10 0.09 0.08 0.08 0.10 0.21 0.08 0.09\n",
      "0.08 0.08 0.08 0.08 0.07 0.09 0.09 0.09 0.26 0.09\n",
      "0.10 0.08 0.10 0.10 0.08 0.08 0.09 0.09 0.08 0.24\n",
      "groundtruth\n",
      "0.20 0.10 0.10 0.07 0.10 0.06 0.06 0.10 0.11 0.10\n",
      "0.07 0.17 0.09 0.13 0.09 0.08 0.08 0.07 0.11 0.07\n",
      "0.08 0.06 0.17 0.11 0.09 0.06 0.09 0.11 0.08 0.11\n",
      "0.08 0.08 0.07 0.17 0.10 0.09 0.06 0.07 0.09 0.08\n",
      "0.11 0.08 0.10 0.08 0.18 0.11 0.10 0.08 0.11 0.09\n",
      "0.06 0.11 0.08 0.10 0.10 0.22 0.08 0.10 0.08 0.09\n",
      "0.12 0.11 0.08 0.09 0.09 0.06 0.21 0.08 0.09 0.11\n",
      "0.10 0.11 0.11 0.07 0.09 0.09 0.10 0.21 0.10 0.07\n",
      "0.09 0.10 0.08 0.08 0.06 0.12 0.11 0.11 0.18 0.07\n",
      "0.10 0.07 0.11 0.10 0.10 0.10 0.11 0.07 0.06 0.21\n",
      "Error:0.22\n",
      "---------------\n",
      "Classifier: 3\n",
      "---------------\n",
      "Estimate\n",
      "0.26 0.09 0.09 0.10 0.09 0.09 0.09 0.10 0.08 0.09\n",
      "0.07 0.26 0.08 0.09 0.08 0.08 0.09 0.08 0.08 0.08\n",
      "0.08 0.08 0.20 0.09 0.06 0.07 0.07 0.08 0.06 0.08\n",
      "0.08 0.10 0.10 0.20 0.08 0.10 0.09 0.10 0.09 0.10\n",
      "0.10 0.09 0.09 0.09 0.32 0.09 0.09 0.10 0.08 0.09\n",
      "0.09 0.09 0.09 0.10 0.08 0.26 0.09 0.08 0.09 0.08\n",
      "0.08 0.08 0.08 0.07 0.07 0.07 0.23 0.09 0.07 0.08\n",
      "0.07 0.07 0.09 0.09 0.07 0.08 0.08 0.20 0.08 0.08\n",
      "0.07 0.07 0.09 0.09 0.08 0.08 0.08 0.09 0.29 0.08\n",
      "0.09 0.08 0.09 0.10 0.08 0.09 0.08 0.08 0.08 0.24\n",
      "groundtruth\n",
      "0.30 0.11 0.06 0.08 0.10 0.10 0.10 0.07 0.08 0.08\n",
      "0.07 0.26 0.09 0.09 0.08 0.08 0.09 0.08 0.10 0.08\n",
      "0.07 0.08 0.30 0.09 0.06 0.07 0.06 0.09 0.05 0.09\n",
      "0.07 0.10 0.11 0.26 0.09 0.10 0.06 0.10 0.07 0.08\n",
      "0.11 0.08 0.06 0.07 0.30 0.08 0.07 0.09 0.09 0.07\n",
      "0.09 0.10 0.10 0.09 0.08 0.27 0.11 0.06 0.09 0.07\n",
      "0.09 0.08 0.08 0.05 0.07 0.07 0.27 0.09 0.08 0.10\n",
      "0.06 0.07 0.08 0.09 0.07 0.06 0.06 0.26 0.08 0.09\n",
      "0.07 0.06 0.06 0.09 0.06 0.10 0.11 0.06 0.28 0.09\n",
      "0.07 0.07 0.06 0.09 0.09 0.08 0.08 0.09 0.09 0.26\n",
      "Error:0.20\n",
      "---------------\n",
      "Classifier: 4\n",
      "---------------\n",
      "Estimate\n",
      "0.25 0.09 0.09 0.11 0.08 0.09 0.09 0.10 0.09 0.09\n",
      "0.07 0.25 0.08 0.08 0.07 0.07 0.08 0.08 0.07 0.08\n",
      "0.07 0.08 0.20 0.08 0.08 0.08 0.07 0.08 0.08 0.07\n",
      "0.08 0.08 0.08 0.17 0.08 0.08 0.08 0.09 0.07 0.08\n",
      "0.09 0.09 0.10 0.11 0.30 0.10 0.10 0.09 0.09 0.10\n",
      "0.08 0.08 0.08 0.09 0.08 0.25 0.08 0.10 0.08 0.10\n",
      "0.08 0.07 0.08 0.10 0.08 0.07 0.23 0.10 0.08 0.09\n",
      "0.11 0.09 0.11 0.09 0.09 0.09 0.09 0.20 0.09 0.08\n",
      "0.08 0.08 0.09 0.10 0.07 0.08 0.08 0.09 0.28 0.08\n",
      "0.09 0.08 0.08 0.08 0.09 0.09 0.10 0.08 0.09 0.23\n",
      "groundtruth\n",
      "0.25 0.10 0.09 0.11 0.06 0.10 0.09 0.08 0.10 0.09\n",
      "0.06 0.24 0.09 0.08 0.06 0.07 0.08 0.09 0.08 0.08\n",
      "0.05 0.09 0.25 0.06 0.11 0.10 0.08 0.07 0.10 0.06\n",
      "0.06 0.08 0.06 0.21 0.07 0.10 0.10 0.08 0.06 0.09\n",
      "0.10 0.09 0.10 0.11 0.24 0.10 0.07 0.05 0.11 0.10\n",
      "0.09 0.06 0.10 0.10 0.09 0.23 0.08 0.09 0.07 0.08\n",
      "0.10 0.06 0.06 0.11 0.08 0.06 0.25 0.11 0.07 0.10\n",
      "0.09 0.11 0.08 0.06 0.10 0.08 0.07 0.24 0.10 0.07\n",
      "0.09 0.06 0.08 0.09 0.07 0.08 0.06 0.10 0.22 0.11\n",
      "0.10 0.10 0.09 0.07 0.11 0.08 0.11 0.09 0.09 0.21\n",
      "Error:0.18\n",
      "---------------\n",
      "Classifier: 5\n",
      "---------------\n",
      "Estimate\n",
      "0.25 0.09 0.08 0.09 0.08 0.08 0.08 0.09 0.08 0.08\n",
      "0.08 0.26 0.07 0.09 0.07 0.08 0.09 0.08 0.07 0.08\n",
      "0.07 0.07 0.21 0.07 0.07 0.07 0.06 0.08 0.07 0.07\n",
      "0.09 0.09 0.09 0.20 0.08 0.09 0.10 0.09 0.09 0.09\n",
      "0.08 0.07 0.09 0.09 0.28 0.08 0.09 0.08 0.08 0.08\n",
      "0.09 0.09 0.09 0.09 0.09 0.26 0.10 0.10 0.09 0.10\n",
      "0.08 0.07 0.08 0.08 0.07 0.07 0.21 0.09 0.07 0.08\n",
      "0.09 0.09 0.10 0.09 0.08 0.09 0.08 0.20 0.08 0.09\n",
      "0.08 0.09 0.10 0.09 0.09 0.09 0.09 0.10 0.29 0.09\n",
      "0.08 0.09 0.08 0.10 0.09 0.10 0.10 0.10 0.08 0.25\n",
      "groundtruth\n",
      "0.25 0.09 0.08 0.07 0.08 0.10 0.06 0.08 0.08 0.06\n",
      "0.09 0.22 0.08 0.07 0.10 0.09 0.10 0.09 0.06 0.10\n",
      "0.06 0.08 0.25 0.06 0.10 0.07 0.06 0.08 0.08 0.07\n",
      "0.10 0.09 0.07 0.25 0.07 0.11 0.11 0.07 0.08 0.08\n",
      "0.08 0.08 0.09 0.09 0.21 0.06 0.11 0.07 0.10 0.08\n",
      "0.09 0.10 0.07 0.10 0.11 0.24 0.11 0.11 0.09 0.07\n",
      "0.08 0.07 0.08 0.07 0.09 0.08 0.22 0.09 0.09 0.08\n",
      "0.07 0.06 0.11 0.08 0.07 0.08 0.07 0.24 0.09 0.11\n",
      "0.08 0.10 0.08 0.09 0.09 0.10 0.08 0.09 0.26 0.09\n",
      "0.08 0.11 0.08 0.10 0.09 0.08 0.09 0.09 0.06 0.26\n",
      "Error:0.17\n",
      "---------------\n",
      "Classifier: 6\n",
      "---------------\n",
      "Estimate\n",
      "0.25 0.09 0.09 0.09 0.08 0.07 0.08 0.09 0.08 0.08\n",
      "0.08 0.26 0.08 0.08 0.07 0.08 0.08 0.08 0.08 0.09\n",
      "0.09 0.08 0.20 0.10 0.07 0.08 0.07 0.08 0.08 0.08\n",
      "0.08 0.08 0.09 0.19 0.07 0.08 0.09 0.09 0.08 0.09\n",
      "0.09 0.08 0.09 0.11 0.30 0.09 0.10 0.10 0.08 0.08\n",
      "0.08 0.09 0.08 0.08 0.08 0.26 0.09 0.10 0.08 0.09\n",
      "0.07 0.08 0.08 0.09 0.08 0.08 0.23 0.08 0.07 0.08\n",
      "0.10 0.09 0.10 0.09 0.09 0.09 0.08 0.21 0.09 0.09\n",
      "0.07 0.07 0.09 0.09 0.09 0.08 0.08 0.08 0.28 0.08\n",
      "0.09 0.08 0.09 0.08 0.08 0.08 0.09 0.09 0.08 0.25\n",
      "groundtruth\n",
      "0.26 0.09 0.06 0.07 0.09 0.06 0.06 0.09 0.09 0.08\n",
      "0.08 0.27 0.11 0.07 0.08 0.10 0.09 0.08 0.06 0.08\n",
      "0.10 0.08 0.28 0.11 0.06 0.09 0.09 0.08 0.09 0.07\n",
      "0.06 0.11 0.07 0.26 0.06 0.08 0.08 0.08 0.07 0.09\n",
      "0.08 0.06 0.10 0.10 0.31 0.09 0.09 0.07 0.10 0.07\n",
      "0.08 0.09 0.09 0.08 0.07 0.27 0.09 0.10 0.07 0.10\n",
      "0.07 0.10 0.06 0.09 0.07 0.07 0.28 0.07 0.06 0.08\n",
      "0.10 0.08 0.06 0.07 0.10 0.06 0.07 0.26 0.09 0.08\n",
      "0.08 0.06 0.06 0.07 0.09 0.09 0.09 0.09 0.29 0.09\n",
      "0.11 0.06 0.11 0.08 0.07 0.08 0.07 0.08 0.08 0.27\n",
      "Error:0.18\n",
      "---------------\n",
      "Classifier: 7\n",
      "---------------\n",
      "Estimate\n",
      "0.24 0.09 0.09 0.10 0.08 0.08 0.08 0.09 0.08 0.09\n",
      "0.08 0.26 0.08 0.09 0.07 0.07 0.08 0.09 0.07 0.07\n",
      "0.08 0.07 0.21 0.08 0.07 0.07 0.07 0.09 0.07 0.08\n",
      "0.09 0.08 0.09 0.19 0.08 0.08 0.10 0.09 0.09 0.10\n",
      "0.09 0.08 0.09 0.10 0.29 0.08 0.10 0.09 0.08 0.08\n",
      "0.09 0.09 0.08 0.08 0.08 0.26 0.08 0.09 0.08 0.08\n",
      "0.08 0.09 0.09 0.10 0.09 0.10 0.23 0.10 0.08 0.10\n",
      "0.09 0.08 0.09 0.10 0.08 0.08 0.09 0.20 0.08 0.09\n",
      "0.08 0.08 0.09 0.10 0.08 0.08 0.09 0.08 0.27 0.08\n",
      "0.08 0.08 0.08 0.08 0.08 0.09 0.08 0.08 0.07 0.23\n",
      "groundtruth\n",
      "0.22 0.09 0.08 0.09 0.08 0.06 0.10 0.10 0.09 0.09\n",
      "0.11 0.21 0.06 0.08 0.09 0.05 0.08 0.07 0.09 0.09\n",
      "0.06 0.06 0.27 0.07 0.08 0.09 0.09 0.10 0.08 0.08\n",
      "0.09 0.07 0.07 0.23 0.09 0.07 0.07 0.11 0.10 0.09\n",
      "0.09 0.07 0.10 0.10 0.21 0.09 0.10 0.08 0.10 0.10\n",
      "0.11 0.11 0.07 0.09 0.11 0.25 0.06 0.08 0.07 0.05\n",
      "0.07 0.10 0.10 0.10 0.11 0.10 0.23 0.10 0.10 0.09\n",
      "0.06 0.10 0.08 0.09 0.08 0.10 0.10 0.21 0.07 0.11\n",
      "0.07 0.10 0.06 0.10 0.08 0.10 0.10 0.07 0.22 0.06\n",
      "0.10 0.09 0.11 0.06 0.07 0.09 0.08 0.09 0.08 0.25\n",
      "Error:0.19\n",
      "---------------\n",
      "Classifier: 8\n",
      "---------------\n",
      "Estimate\n",
      "0.23 0.08 0.09 0.08 0.07 0.09 0.08 0.09 0.08 0.08\n",
      "0.07 0.25 0.08 0.08 0.07 0.08 0.09 0.08 0.08 0.08\n",
      "0.07 0.07 0.21 0.09 0.07 0.07 0.07 0.09 0.07 0.06\n",
      "0.10 0.09 0.09 0.18 0.08 0.09 0.10 0.10 0.09 0.10\n",
      "0.10 0.09 0.10 0.10 0.29 0.10 0.10 0.11 0.09 0.10\n",
      "0.08 0.09 0.09 0.09 0.08 0.25 0.08 0.09 0.08 0.08\n",
      "0.08 0.08 0.08 0.10 0.08 0.08 0.22 0.09 0.09 0.09\n",
      "0.10 0.08 0.10 0.10 0.09 0.09 0.09 0.18 0.09 0.09\n",
      "0.08 0.08 0.09 0.09 0.08 0.07 0.07 0.08 0.26 0.08\n",
      "0.09 0.09 0.08 0.09 0.08 0.09 0.09 0.09 0.07 0.23\n",
      "groundtruth\n",
      "0.18 0.08 0.10 0.07 0.07 0.11 0.10 0.10 0.07 0.09\n",
      "0.10 0.16 0.09 0.07 0.08 0.08 0.07 0.07 0.11 0.09\n",
      "0.08 0.06 0.20 0.09 0.08 0.07 0.07 0.12 0.07 0.07\n",
      "0.10 0.10 0.09 0.21 0.07 0.10 0.12 0.10 0.09 0.10\n",
      "0.10 0.11 0.08 0.09 0.19 0.09 0.08 0.09 0.11 0.10\n",
      "0.08 0.11 0.08 0.12 0.11 0.20 0.07 0.09 0.08 0.06\n",
      "0.07 0.07 0.06 0.11 0.10 0.09 0.20 0.10 0.10 0.09\n",
      "0.09 0.09 0.09 0.08 0.13 0.07 0.09 0.16 0.13 0.09\n",
      "0.09 0.11 0.11 0.09 0.08 0.09 0.09 0.07 0.18 0.10\n",
      "0.10 0.11 0.09 0.08 0.08 0.10 0.11 0.09 0.07 0.20\n",
      "Error:0.23\n",
      "---------------\n",
      "Classifier: 9\n",
      "---------------\n",
      "Estimate\n",
      "0.23 0.07 0.08 0.08 0.08 0.08 0.08 0.08 0.08 0.09\n",
      "0.09 0.26 0.09 0.10 0.08 0.08 0.09 0.09 0.08 0.09\n",
      "0.08 0.08 0.19 0.09 0.07 0.08 0.08 0.08 0.07 0.07\n",
      "0.09 0.09 0.10 0.18 0.08 0.09 0.09 0.09 0.09 0.10\n",
      "0.09 0.09 0.09 0.10 0.30 0.08 0.09 0.09 0.08 0.09\n",
      "0.08 0.08 0.08 0.08 0.07 0.25 0.08 0.08 0.07 0.08\n",
      "0.09 0.08 0.09 0.10 0.08 0.08 0.23 0.10 0.08 0.08\n",
      "0.09 0.09 0.10 0.09 0.09 0.09 0.09 0.21 0.09 0.09\n",
      "0.09 0.08 0.09 0.09 0.08 0.08 0.08 0.09 0.27 0.09\n",
      "0.08 0.08 0.09 0.09 0.08 0.08 0.09 0.09 0.08 0.23\n",
      "groundtruth\n",
      "0.19 0.07 0.09 0.09 0.06 0.10 0.09 0.07 0.07 0.11\n",
      "0.11 0.17 0.10 0.09 0.10 0.11 0.07 0.10 0.09 0.09\n",
      "0.11 0.12 0.21 0.11 0.07 0.07 0.08 0.07 0.07 0.07\n",
      "0.08 0.11 0.07 0.21 0.11 0.10 0.07 0.08 0.11 0.09\n",
      "0.07 0.09 0.10 0.08 0.20 0.10 0.11 0.07 0.12 0.11\n",
      "0.11 0.10 0.10 0.07 0.08 0.20 0.09 0.07 0.07 0.06\n",
      "0.09 0.11 0.06 0.10 0.11 0.06 0.21 0.11 0.07 0.10\n",
      "0.09 0.07 0.11 0.07 0.11 0.09 0.10 0.22 0.13 0.08\n",
      "0.09 0.09 0.08 0.08 0.07 0.10 0.09 0.11 0.18 0.10\n",
      "0.07 0.08 0.08 0.10 0.09 0.06 0.09 0.10 0.09 0.19\n",
      "Error:0.24\n",
      "---------------\n",
      "Classifier: 10\n",
      "---------------\n",
      "Estimate\n",
      "0.26 0.09 0.09 0.10 0.08 0.08 0.07 0.09 0.08 0.08\n",
      "0.08 0.26 0.08 0.08 0.08 0.08 0.08 0.08 0.08 0.08\n",
      "0.08 0.08 0.21 0.08 0.08 0.07 0.08 0.09 0.08 0.08\n",
      "0.09 0.09 0.08 0.20 0.08 0.08 0.09 0.09 0.09 0.09\n",
      "0.10 0.10 0.11 0.11 0.31 0.09 0.09 0.11 0.09 0.08\n",
      "0.08 0.08 0.08 0.07 0.07 0.25 0.09 0.08 0.07 0.09\n",
      "0.07 0.07 0.08 0.08 0.08 0.07 0.23 0.07 0.07 0.08\n",
      "0.09 0.09 0.10 0.09 0.08 0.10 0.09 0.22 0.09 0.09\n",
      "0.08 0.09 0.09 0.10 0.08 0.08 0.10 0.09 0.29 0.08\n",
      "0.08 0.08 0.08 0.08 0.07 0.08 0.08 0.08 0.07 0.24\n",
      "groundtruth\n",
      "0.28 0.09 0.09 0.10 0.08 0.10 0.07 0.08 0.06 0.08\n",
      "0.08 0.27 0.07 0.07 0.09 0.08 0.06 0.08 0.08 0.06\n",
      "0.08 0.07 0.29 0.08 0.09 0.08 0.05 0.10 0.10 0.11\n",
      "0.11 0.08 0.06 0.30 0.06 0.10 0.10 0.07 0.09 0.07\n",
      "0.09 0.09 0.09 0.08 0.29 0.07 0.10 0.10 0.10 0.07\n",
      "0.08 0.08 0.09 0.06 0.09 0.25 0.07 0.06 0.07 0.11\n",
      "0.08 0.07 0.09 0.06 0.08 0.08 0.26 0.07 0.10 0.08\n",
      "0.06 0.09 0.08 0.10 0.09 0.08 0.11 0.28 0.07 0.08\n",
      "0.07 0.07 0.07 0.10 0.08 0.09 0.11 0.06 0.27 0.09\n",
      "0.06 0.10 0.07 0.06 0.05 0.08 0.08 0.09 0.06 0.26\n",
      "Error:0.20\n",
      "---------------\n",
      "Classifier: 11\n",
      "---------------\n",
      "Estimate\n",
      "0.26 0.09 0.09 0.08 0.08 0.09 0.09 0.09 0.08 0.09\n",
      "0.08 0.27 0.09 0.09 0.08 0.08 0.09 0.09 0.08 0.08\n",
      "0.07 0.08 0.21 0.08 0.07 0.08 0.08 0.08 0.07 0.08\n",
      "0.09 0.08 0.10 0.22 0.09 0.10 0.09 0.09 0.09 0.09\n",
      "0.08 0.07 0.08 0.10 0.30 0.07 0.08 0.09 0.07 0.08\n",
      "0.09 0.09 0.08 0.10 0.07 0.26 0.08 0.08 0.08 0.08\n",
      "0.08 0.08 0.08 0.08 0.07 0.08 0.24 0.09 0.07 0.07\n",
      "0.09 0.08 0.09 0.09 0.08 0.09 0.09 0.21 0.08 0.09\n",
      "0.09 0.09 0.09 0.09 0.08 0.08 0.09 0.10 0.30 0.08\n",
      "0.08 0.08 0.08 0.08 0.08 0.08 0.08 0.08 0.07 0.25\n",
      "groundtruth\n",
      "0.29 0.09 0.07 0.07 0.08 0.09 0.09 0.08 0.06 0.09\n",
      "0.07 0.29 0.10 0.10 0.07 0.08 0.09 0.09 0.06 0.08\n",
      "0.06 0.10 0.30 0.06 0.07 0.09 0.09 0.07 0.08 0.10\n",
      "0.10 0.06 0.07 0.29 0.09 0.10 0.08 0.06 0.08 0.07\n",
      "0.07 0.07 0.10 0.09 0.26 0.06 0.06 0.07 0.07 0.10\n",
      "0.08 0.08 0.09 0.09 0.06 0.27 0.07 0.07 0.09 0.06\n",
      "0.07 0.07 0.06 0.07 0.09 0.08 0.27 0.10 0.08 0.06\n",
      "0.08 0.07 0.08 0.07 0.10 0.10 0.10 0.27 0.10 0.09\n",
      "0.09 0.09 0.06 0.07 0.10 0.08 0.08 0.10 0.30 0.06\n",
      "0.09 0.07 0.07 0.08 0.09 0.05 0.06 0.09 0.07 0.30\n",
      "Error:0.20\n",
      "---------------\n",
      "Classifier: 12\n",
      "---------------\n",
      "Estimate\n",
      "0.25 0.09 0.09 0.10 0.08 0.10 0.09 0.09 0.08 0.10\n",
      "0.10 0.24 0.09 0.10 0.08 0.08 0.08 0.08 0.08 0.08\n",
      "0.08 0.08 0.20 0.10 0.07 0.08 0.08 0.10 0.08 0.07\n",
      "0.09 0.09 0.10 0.20 0.09 0.10 0.08 0.09 0.08 0.09\n",
      "0.08 0.08 0.08 0.08 0.30 0.07 0.09 0.08 0.09 0.08\n",
      "0.09 0.09 0.09 0.10 0.09 0.27 0.09 0.09 0.09 0.08\n",
      "0.07 0.07 0.07 0.08 0.06 0.07 0.22 0.07 0.06 0.06\n",
      "0.08 0.10 0.09 0.09 0.09 0.08 0.10 0.23 0.08 0.10\n",
      "0.09 0.09 0.08 0.09 0.07 0.09 0.09 0.09 0.29 0.09\n",
      "0.08 0.07 0.09 0.07 0.07 0.07 0.08 0.07 0.07 0.25\n",
      "groundtruth\n",
      "0.26 0.06 0.09 0.09 0.08 0.11 0.08 0.09 0.08 0.11\n",
      "0.10 0.29 0.10 0.09 0.07 0.10 0.07 0.07 0.09 0.10\n",
      "0.06 0.11 0.26 0.11 0.07 0.10 0.07 0.11 0.10 0.06\n",
      "0.09 0.10 0.09 0.26 0.10 0.07 0.07 0.09 0.05 0.09\n",
      "0.09 0.06 0.06 0.06 0.28 0.06 0.10 0.06 0.10 0.07\n",
      "0.07 0.09 0.10 0.09 0.11 0.29 0.07 0.09 0.09 0.09\n",
      "0.06 0.07 0.07 0.08 0.07 0.06 0.28 0.08 0.06 0.06\n",
      "0.10 0.07 0.07 0.08 0.08 0.09 0.11 0.27 0.06 0.08\n",
      "0.11 0.10 0.05 0.09 0.06 0.07 0.08 0.08 0.30 0.06\n",
      "0.07 0.06 0.10 0.06 0.07 0.07 0.08 0.06 0.06 0.29\n",
      "Error:0.18\n",
      "---------------\n",
      "Classifier: 13\n",
      "---------------\n",
      "Estimate\n",
      "0.25 0.09 0.09 0.10 0.08 0.09 0.09 0.10 0.09 0.09\n",
      "0.10 0.28 0.10 0.09 0.08 0.10 0.09 0.10 0.09 0.09\n",
      "0.08 0.07 0.20 0.07 0.07 0.07 0.08 0.08 0.07 0.08\n",
      "0.08 0.08 0.09 0.18 0.09 0.09 0.09 0.09 0.09 0.09\n",
      "0.08 0.07 0.08 0.10 0.27 0.08 0.09 0.09 0.08 0.08\n",
      "0.08 0.08 0.08 0.08 0.07 0.24 0.08 0.08 0.07 0.08\n",
      "0.08 0.08 0.08 0.11 0.08 0.09 0.22 0.09 0.08 0.08\n",
      "0.08 0.08 0.09 0.08 0.08 0.09 0.09 0.17 0.08 0.08\n",
      "0.09 0.09 0.10 0.11 0.10 0.08 0.09 0.10 0.28 0.10\n",
      "0.07 0.08 0.09 0.08 0.07 0.09 0.08 0.09 0.07 0.23\n",
      "groundtruth\n",
      "0.22 0.08 0.08 0.10 0.08 0.11 0.09 0.10 0.10 0.09\n",
      "0.11 0.21 0.11 0.07 0.10 0.10 0.08 0.11 0.09 0.11\n",
      "0.09 0.09 0.22 0.06 0.07 0.08 0.09 0.07 0.07 0.11\n",
      "0.10 0.07 0.07 0.20 0.12 0.07 0.07 0.11 0.12 0.07\n",
      "0.08 0.08 0.11 0.08 0.17 0.08 0.10 0.09 0.09 0.09\n",
      "0.06 0.11 0.08 0.10 0.08 0.19 0.11 0.07 0.07 0.10\n",
      "0.11 0.11 0.07 0.11 0.12 0.11 0.18 0.08 0.07 0.06\n",
      "0.07 0.08 0.09 0.07 0.09 0.08 0.10 0.17 0.12 0.09\n",
      "0.09 0.09 0.08 0.11 0.12 0.07 0.10 0.11 0.17 0.10\n",
      "0.07 0.09 0.10 0.09 0.07 0.11 0.08 0.09 0.10 0.19\n",
      "Error:0.24\n",
      "---------------\n",
      "Classifier: 14\n",
      "---------------\n",
      "Estimate\n",
      "0.24 0.07 0.08 0.08 0.07 0.08 0.09 0.08 0.07 0.08\n",
      "0.08 0.24 0.09 0.08 0.07 0.08 0.08 0.08 0.06 0.08\n",
      "0.07 0.07 0.20 0.08 0.08 0.07 0.07 0.08 0.08 0.08\n",
      "0.08 0.08 0.10 0.18 0.09 0.09 0.08 0.09 0.08 0.08\n",
      "0.09 0.09 0.10 0.10 0.31 0.09 0.10 0.09 0.09 0.09\n",
      "0.09 0.08 0.10 0.11 0.08 0.26 0.08 0.09 0.09 0.09\n",
      "0.09 0.10 0.08 0.09 0.08 0.09 0.23 0.10 0.08 0.08\n",
      "0.08 0.09 0.09 0.09 0.08 0.08 0.10 0.21 0.09 0.10\n",
      "0.09 0.08 0.08 0.09 0.07 0.08 0.08 0.08 0.28 0.08\n",
      "0.08 0.08 0.09 0.10 0.08 0.08 0.08 0.09 0.08 0.24\n",
      "groundtruth\n",
      "0.23 0.06 0.07 0.08 0.08 0.08 0.10 0.08 0.06 0.10\n",
      "0.06 0.23 0.10 0.08 0.07 0.09 0.10 0.07 0.06 0.07\n",
      "0.10 0.08 0.25 0.09 0.10 0.09 0.08 0.08 0.09 0.08\n",
      "0.06 0.06 0.10 0.22 0.10 0.10 0.08 0.09 0.08 0.06\n",
      "0.09 0.11 0.08 0.07 0.24 0.09 0.08 0.09 0.11 0.10\n",
      "0.10 0.10 0.10 0.09 0.06 0.25 0.08 0.09 0.09 0.12\n",
      "0.10 0.09 0.07 0.07 0.11 0.09 0.25 0.12 0.08 0.08\n",
      "0.10 0.06 0.09 0.08 0.10 0.07 0.09 0.21 0.10 0.11\n",
      "0.10 0.10 0.08 0.12 0.06 0.06 0.09 0.08 0.25 0.06\n",
      "0.07 0.11 0.06 0.11 0.08 0.09 0.06 0.08 0.08 0.22\n",
      "Error:0.17\n",
      "---------------\n",
      "Classifier: 15\n",
      "---------------\n",
      "Estimate\n",
      "0.24 0.08 0.10 0.09 0.08 0.08 0.09 0.09 0.09 0.09\n",
      "0.08 0.27 0.08 0.09 0.09 0.09 0.09 0.09 0.09 0.08\n",
      "0.08 0.08 0.21 0.08 0.07 0.08 0.08 0.09 0.07 0.09\n",
      "0.08 0.09 0.10 0.19 0.08 0.09 0.10 0.09 0.09 0.09\n",
      "0.08 0.07 0.08 0.10 0.29 0.08 0.09 0.09 0.08 0.09\n",
      "0.08 0.07 0.09 0.08 0.07 0.25 0.08 0.08 0.07 0.09\n",
      "0.08 0.08 0.08 0.08 0.08 0.08 0.22 0.08 0.07 0.09\n",
      "0.11 0.09 0.10 0.10 0.09 0.09 0.09 0.21 0.08 0.08\n",
      "0.08 0.08 0.08 0.09 0.07 0.07 0.08 0.08 0.28 0.08\n",
      "0.09 0.08 0.08 0.09 0.08 0.09 0.09 0.09 0.08 0.23\n",
      "groundtruth\n",
      "0.24 0.08 0.11 0.11 0.10 0.09 0.07 0.06 0.09 0.08\n",
      "0.09 0.24 0.08 0.07 0.10 0.06 0.12 0.10 0.10 0.07\n",
      "0.08 0.09 0.22 0.10 0.07 0.10 0.09 0.09 0.09 0.10\n",
      "0.06 0.09 0.07 0.23 0.10 0.10 0.06 0.09 0.10 0.09\n",
      "0.07 0.08 0.07 0.08 0.22 0.10 0.12 0.08 0.09 0.09\n",
      "0.07 0.09 0.09 0.07 0.09 0.24 0.08 0.10 0.06 0.10\n",
      "0.11 0.07 0.07 0.07 0.08 0.10 0.21 0.09 0.07 0.08\n",
      "0.11 0.08 0.11 0.09 0.10 0.05 0.08 0.24 0.09 0.07\n",
      "0.09 0.11 0.09 0.08 0.08 0.06 0.08 0.06 0.23 0.09\n",
      "0.08 0.08 0.08 0.10 0.08 0.09 0.09 0.10 0.09 0.22\n",
      "Error:0.17\n",
      "---------------\n",
      "Classifier: 16\n",
      "---------------\n",
      "Estimate\n",
      "0.26 0.08 0.09 0.09 0.08 0.09 0.08 0.07 0.07 0.08\n",
      "0.07 0.24 0.08 0.08 0.06 0.07 0.07 0.07 0.07 0.08\n",
      "0.07 0.08 0.20 0.08 0.07 0.08 0.08 0.08 0.07 0.08\n",
      "0.10 0.10 0.11 0.21 0.09 0.10 0.09 0.10 0.10 0.09\n",
      "0.08 0.09 0.09 0.09 0.30 0.08 0.08 0.08 0.08 0.08\n",
      "0.09 0.08 0.09 0.08 0.07 0.26 0.08 0.09 0.08 0.08\n",
      "0.09 0.08 0.08 0.09 0.07 0.08 0.23 0.08 0.07 0.08\n",
      "0.08 0.10 0.09 0.10 0.10 0.09 0.10 0.24 0.09 0.10\n",
      "0.08 0.07 0.08 0.09 0.07 0.08 0.08 0.09 0.28 0.08\n",
      "0.08 0.08 0.10 0.09 0.07 0.07 0.09 0.09 0.09 0.25\n",
      "groundtruth\n",
      "0.29 0.08 0.07 0.07 0.10 0.05 0.06 0.06 0.07 0.10\n",
      "0.07 0.28 0.05 0.09 0.08 0.08 0.07 0.06 0.09 0.08\n",
      "0.09 0.09 0.29 0.06 0.07 0.10 0.08 0.09 0.09 0.07\n",
      "0.07 0.09 0.10 0.27 0.09 0.08 0.09 0.10 0.08 0.09\n",
      "0.07 0.07 0.08 0.09 0.28 0.06 0.07 0.06 0.07 0.06\n",
      "0.08 0.10 0.08 0.07 0.06 0.30 0.08 0.09 0.07 0.10\n",
      "0.10 0.06 0.08 0.09 0.09 0.08 0.29 0.09 0.07 0.08\n",
      "0.09 0.10 0.08 0.08 0.10 0.09 0.10 0.29 0.08 0.08\n",
      "0.07 0.06 0.10 0.09 0.06 0.08 0.08 0.07 0.30 0.08\n",
      "0.07 0.07 0.08 0.08 0.08 0.08 0.09 0.09 0.09 0.27\n",
      "Error:0.19\n",
      "---------------\n",
      "Classifier: 17\n",
      "---------------\n",
      "Estimate\n",
      "0.25 0.08 0.09 0.09 0.09 0.08 0.09 0.09 0.08 0.09\n",
      "0.07 0.24 0.09 0.09 0.07 0.07 0.08 0.09 0.07 0.08\n",
      "0.08 0.07 0.20 0.08 0.07 0.08 0.08 0.08 0.07 0.07\n",
      "0.09 0.09 0.10 0.19 0.09 0.10 0.10 0.09 0.08 0.08\n",
      "0.09 0.09 0.09 0.09 0.30 0.08 0.09 0.09 0.09 0.09\n",
      "0.08 0.08 0.09 0.10 0.08 0.26 0.08 0.08 0.07 0.07\n",
      "0.08 0.08 0.08 0.08 0.08 0.08 0.21 0.08 0.08 0.08\n",
      "0.08 0.10 0.09 0.09 0.08 0.09 0.11 0.23 0.09 0.10\n",
      "0.09 0.08 0.09 0.10 0.08 0.09 0.09 0.09 0.28 0.09\n",
      "0.09 0.08 0.09 0.10 0.08 0.08 0.09 0.08 0.08 0.26\n",
      "groundtruth\n",
      "0.29 0.07 0.09 0.05 0.10 0.08 0.06 0.10 0.07 0.10\n",
      "0.09 0.28 0.08 0.09 0.08 0.06 0.05 0.09 0.06 0.08\n",
      "0.08 0.08 0.28 0.08 0.08 0.10 0.06 0.09 0.07 0.09\n",
      "0.07 0.10 0.06 0.28 0.09 0.09 0.10 0.06 0.07 0.06\n",
      "0.09 0.10 0.09 0.07 0.25 0.06 0.09 0.08 0.10 0.10\n",
      "0.06 0.06 0.09 0.10 0.06 0.26 0.10 0.09 0.07 0.05\n",
      "0.07 0.09 0.05 0.07 0.11 0.06 0.30 0.06 0.10 0.09\n",
      "0.10 0.10 0.09 0.08 0.06 0.10 0.08 0.26 0.10 0.06\n",
      "0.08 0.06 0.09 0.10 0.07 0.09 0.09 0.09 0.28 0.10\n",
      "0.08 0.06 0.07 0.09 0.09 0.10 0.07 0.07 0.08 0.28\n",
      "Error:0.22\n",
      "---------------\n",
      "Classifier: 18\n",
      "---------------\n",
      "Estimate\n",
      "0.24 0.08 0.09 0.08 0.08 0.08 0.09 0.09 0.08 0.10\n",
      "0.08 0.26 0.09 0.09 0.08 0.09 0.09 0.09 0.07 0.08\n",
      "0.08 0.08 0.20 0.09 0.07 0.07 0.08 0.08 0.08 0.08\n",
      "0.10 0.09 0.10 0.17 0.09 0.09 0.08 0.09 0.08 0.10\n",
      "0.09 0.08 0.09 0.09 0.30 0.09 0.09 0.10 0.08 0.08\n",
      "0.08 0.08 0.09 0.10 0.07 0.25 0.09 0.09 0.08 0.09\n",
      "0.08 0.08 0.09 0.08 0.08 0.08 0.23 0.09 0.08 0.08\n",
      "0.09 0.09 0.08 0.09 0.07 0.09 0.08 0.19 0.08 0.07\n",
      "0.09 0.09 0.10 0.11 0.09 0.10 0.10 0.10 0.30 0.10\n",
      "0.07 0.07 0.07 0.08 0.07 0.07 0.07 0.08 0.07 0.22\n",
      "groundtruth\n",
      "0.21 0.06 0.11 0.07 0.10 0.10 0.09 0.07 0.08 0.12\n",
      "0.08 0.18 0.08 0.09 0.11 0.09 0.11 0.09 0.06 0.08\n",
      "0.06 0.11 0.19 0.10 0.07 0.08 0.07 0.08 0.09 0.10\n",
      "0.11 0.10 0.09 0.20 0.09 0.08 0.09 0.09 0.06 0.12\n",
      "0.06 0.09 0.11 0.08 0.20 0.10 0.07 0.13 0.10 0.06\n",
      "0.11 0.09 0.10 0.13 0.07 0.21 0.09 0.07 0.11 0.07\n",
      "0.07 0.08 0.10 0.07 0.11 0.08 0.21 0.11 0.08 0.10\n",
      "0.11 0.09 0.07 0.07 0.07 0.09 0.08 0.20 0.09 0.07\n",
      "0.11 0.10 0.09 0.12 0.11 0.10 0.12 0.10 0.21 0.11\n",
      "0.07 0.09 0.06 0.08 0.06 0.08 0.06 0.08 0.11 0.17\n",
      "Error:0.22\n",
      "---------------\n",
      "Classifier: 19\n",
      "---------------\n",
      "Estimate\n",
      "0.24 0.08 0.09 0.09 0.08 0.08 0.08 0.09 0.07 0.08\n",
      "0.09 0.26 0.08 0.08 0.07 0.08 0.09 0.09 0.07 0.08\n",
      "0.07 0.08 0.20 0.08 0.07 0.07 0.07 0.07 0.07 0.07\n",
      "0.10 0.08 0.09 0.20 0.09 0.09 0.10 0.09 0.10 0.10\n",
      "0.08 0.08 0.09 0.09 0.29 0.09 0.09 0.09 0.09 0.09\n",
      "0.08 0.09 0.10 0.08 0.07 0.26 0.08 0.09 0.08 0.10\n",
      "0.07 0.07 0.07 0.08 0.07 0.07 0.22 0.08 0.08 0.07\n",
      "0.09 0.09 0.11 0.09 0.08 0.09 0.09 0.21 0.09 0.09\n",
      "0.10 0.08 0.09 0.10 0.09 0.08 0.09 0.09 0.28 0.08\n",
      "0.09 0.09 0.08 0.09 0.09 0.09 0.10 0.10 0.08 0.24\n",
      "groundtruth\n",
      "0.22 0.07 0.08 0.08 0.12 0.10 0.09 0.09 0.06 0.07\n",
      "0.11 0.25 0.10 0.06 0.07 0.08 0.09 0.07 0.09 0.10\n",
      "0.11 0.08 0.24 0.07 0.09 0.07 0.08 0.07 0.08 0.07\n",
      "0.09 0.11 0.07 0.25 0.08 0.09 0.09 0.07 0.11 0.10\n",
      "0.07 0.08 0.05 0.09 0.21 0.10 0.08 0.10 0.11 0.08\n",
      "0.07 0.07 0.11 0.09 0.07 0.25 0.07 0.10 0.08 0.11\n",
      "0.06 0.10 0.07 0.07 0.07 0.06 0.24 0.08 0.10 0.07\n",
      "0.07 0.11 0.10 0.07 0.10 0.08 0.07 0.26 0.07 0.09\n",
      "0.11 0.06 0.08 0.12 0.10 0.07 0.08 0.06 0.24 0.08\n",
      "0.09 0.06 0.09 0.10 0.09 0.10 0.10 0.10 0.07 0.23\n",
      "Error:0.18\n"
     ]
    }
   ],
   "source": [
    "print \"**Individual Confusion Matrix**\"\n",
    "C = {}\n",
    "for j in range(3):\n",
    "    c = perms[j][2]; a = perms[j][0];\n",
    "    P1 = np.linalg.inv(np.dot(np.diag(ph_est_avg), mu_est[a].T))\n",
    "    grp = [x for x in range(num_classifier) if g[c][x]]\n",
    "    for i in grp: \n",
    "        P2_arr = [np.tensordot(labels[l,i,:], Z[a][l,:], axes = 0) for l in range(num_data)]\n",
    "        P2 = np.mean(P2_arr, axis = 0)\n",
    "        C[i] = np.dot(P2, P1)\n",
    "        C[i] = np.array([normP(C[i][:,j]) for j in range(rank)])\n",
    "err = {}\n",
    "for i in range(num_classifier):\n",
    "    print '---------------'\n",
    "    print 'Classifier:',i\n",
    "    print '---------------'\n",
    "    print 'Estimate'\n",
    "    print '%s'%\"\\n\".join(\"%s\"%str_arr(C[i][:,j]) for j in range(rank))\n",
    "    print 'groundtruth'\n",
    "    print '%s'%\"\\n\".join(\"%s\"%str_arr(conf_dict[i][:,j]) for j in range(rank))\n",
    "    err[i] = np.linalg.norm(conf_dict[i]- C[i])\n",
    "    print 'Error:%.2f'%err[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-shot estimation of the labels using EM after spectral initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_hat(labels, C):\n",
    "    # for a particular time slot\n",
    "    # labels[i,:]: the one hot encoding of label of classifier i\n",
    "    # C: dictionary of confusion matrices\n",
    "    arr = np.sum(np.dot(np.log(C[i]), labels[i,:]) for i in range(num_classifier))\n",
    "    wghts = np.exp(arr-max(arr))\n",
    "    q_hat = normP(wghts)\n",
    "    return q_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after Spectral + 0 EM steps:0.57850\n",
      "Accuracy after Spectral + 1 EM steps:0.57600\n",
      "Accuracy after Spectral + 2 EM steps:0.54850\n",
      "Accuracy after Spectral + 3 EM steps:0.53350\n",
      "Accuracy after Spectral + 4 EM steps:0.52150\n",
      "Accuracy after Spectral + 5 EM steps:0.52750\n",
      "Accuracy after Spectral + 6 EM steps:0.49600\n",
      "Accuracy after Spectral + 7 EM steps:0.47650\n",
      "Accuracy after Spectral + 8 EM steps:0.45550\n",
      "Accuracy after Spectral + 9 EM steps:0.43800\n",
      "Accuracy after Spectral + 10 EM steps:0.42200\n",
      "Accuracy after Spectral + 11 EM steps:0.40350\n",
      "Accuracy after Spectral + 12 EM steps:0.41400\n",
      "Accuracy after Spectral + 13 EM steps:0.39850\n",
      "Accuracy after Spectral + 14 EM steps:0.41400\n",
      "Accuracy after Spectral + 15 EM steps:0.43300\n",
      "Accuracy after Spectral + 16 EM steps:0.42000\n",
      "Accuracy after Spectral + 17 EM steps:0.43000\n",
      "Accuracy after Spectral + 18 EM steps:0.41850\n",
      "Accuracy after Spectral + 19 EM steps:0.42100\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "EMiter = 20; batch_size = 2000; \n",
    "# Initialize\n",
    "C_iter = {k:v for k,v in C.iteritems()}\n",
    "# iteration\n",
    "for ix in range(EMiter):\n",
    "    samp_data = np.random.choice(num_data, min(batch_size, num_data))  # Full data\n",
    "    true_label_iter = np.array([true_label[j] for j in samp_data])\n",
    "    labels_iter = np.array([labels[j,:,:] for j in samp_data])\n",
    "    # E-step\n",
    "    q_hat_arr = np.array([compute_q_hat(labels[j,:,:], C_iter) for j in samp_data])\n",
    "    # Estimate labels\n",
    "    est_label = np.argmax(q_hat_arr, axis = 1)\n",
    "    acc = np.mean(est_label== true_label_iter)\n",
    "    print 'Accuracy after Spectral + %d EM steps:%.5f'%(ix,acc)\n",
    "    # M-step \n",
    "    for i in range(num_classifier):\n",
    "        mu_int = np.dot(q_hat_arr.T, labels_iter[:,i,:] )\n",
    "        # normalize the vectors accross rows\n",
    "        C_iter[i] = np.divide(mu_int.astype(float), mu_int.sum(axis = 1, keepdims = True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 0-th EM steps:0.53650\n",
      "Accuracy after 1-th EM steps:0.60100\n",
      "Accuracy after 2-th EM steps:0.56850\n",
      "Accuracy after 3-th EM steps:0.58000\n",
      "Accuracy after 4-th EM steps:0.56950\n",
      "Accuracy after 5-th EM steps:0.54000\n",
      "Accuracy after 6-th EM steps:0.54450\n",
      "Accuracy after 7-th EM steps:0.51600\n",
      "Accuracy after 8-th EM steps:0.51750\n",
      "Accuracy after 9-th EM steps:0.49050\n",
      "Accuracy after 10-th EM steps:0.44200\n",
      "Accuracy after 11-th EM steps:0.45800\n",
      "Accuracy after 12-th EM steps:0.43100\n",
      "Accuracy after 13-th EM steps:0.41800\n",
      "Accuracy after 14-th EM steps:0.40800\n",
      "Accuracy after 15-th EM steps:0.41250\n",
      "Accuracy after 16-th EM steps:0.38900\n",
      "Accuracy after 17-th EM steps:0.39200\n",
      "Accuracy after 18-th EM steps:0.41750\n",
      "Accuracy after 19-th EM steps:0.40300\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "EMiter = 20; batch_size = 2000; alpha_guess = 0.1;\n",
    "# Initialize\n",
    "C_iterEM = {k:genConfMat(rank, alpha_guess) for k in range(num_classifier)}\n",
    "# iteration\n",
    "for ix in range(EMiter):\n",
    "    samp_data = np.random.choice(num_data, min(batch_size, num_data))  # Full data\n",
    "    true_label_iter = np.array([true_label[j] for j in samp_data])\n",
    "    labels_iter = np.array([labels[j,:,:] for j in samp_data])\n",
    "    # E-step\n",
    "    q_hat_arr = np.array([compute_q_hat(labels[j,:,:], C_iterEM) for j in samp_data])\n",
    "    # Estimate labels\n",
    "    est_label = np.argmax(q_hat_arr, axis = 1)\n",
    "    acc = np.mean(est_label== true_label_iter)\n",
    "    print 'Accuracy after %d-th EM steps:%.5f'%(ix,acc)\n",
    "    # M-step \n",
    "    for i in range(num_classifier):\n",
    "        mu_int = np.dot(q_hat_arr.T, labels_iter[:,i,:])\n",
    "        # normalize the vectors accross rows\n",
    "        C_iterEM[i] = np.divide(mu_int.astype(float), mu_int.sum(axis = 1, keepdims = True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Final E-step and Estimation for the whole data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = [not x for x in adv_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Spectral + EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Final accuracy w/ adversarial i.p.(Spectral + 12 EM steps):0.41865\n",
      "**Final accuracy w/o adversarial i.p.(Spectral + 12 EM steps):0.46545\n"
     ]
    }
   ],
   "source": [
    "# E-step\n",
    "q_hat_arr = np.array([compute_q_hat(labels_test[j,:,:], C_iter) for j in range(num_data)])\n",
    "# Estimate labels\n",
    "est_label = np.argmax(q_hat_arr, axis = 1)\n",
    "acc_w_adv = np.mean(est_label == true_label_test)\n",
    "acc_wo_adv = np.mean(est_label[real_test] == true_label_test[real_test])\n",
    "print '**Final accuracy w/ adversarial i.p.(Spectral + %d EM steps):%.5f'%(i_best,acc_w_adv)\n",
    "print '**Final accuracy w/o adversarial i.p.(Spectral + %d EM steps):%.5f'%(i_best,acc_wo_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Final accuracy w/ adversarial i.p. (only Spectral):0.59680\n",
      "**Final accuracy w/o adversarial i.p. (only Spectral):0.66352\n"
     ]
    }
   ],
   "source": [
    "q_hat_arr = np.array([compute_q_hat(labels_test[j,:,:], C) for j in range(num_data)])\n",
    "# Estimate labels\n",
    "est_label = np.argmax(q_hat_arr, axis = 1)\n",
    "acc_w_adv = np.mean(est_label == true_label_test)\n",
    "acc_wo_adv = np.mean(est_label[real_test] == true_label_test[real_test])\n",
    "print '**Final accuracy w/ adversarial i.p. (only Spectral):%.5f'%(acc_w_adv)\n",
    "print '**Final accuracy w/o adversarial i.p. (only Spectral):%.5f'%(acc_wo_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Final accuracy  w/ adversarial i.p. (only EM):0.41720\n",
      "**Final accuracy w/o adversarial i.p. (only EM):0.46384\n"
     ]
    }
   ],
   "source": [
    "q_hat_arr = np.array([compute_q_hat(labels_test[j,:,:], C_iterEM) for j in range(num_data)])\n",
    "# Estimate labels\n",
    "est_label = np.argmax(q_hat_arr, axis = 1)\n",
    "acc_w_adv = np.mean(est_label == true_label_test)\n",
    "acc_wo_adv = np.mean(est_label[real_test] == true_label_test[real_test])\n",
    "print '**Final accuracy  w/ adversarial i.p. (only EM):%.5f'%(acc_w_adv)\n",
    "print '**Final accuracy w/o adversarial i.p. (only EM):%.5f'%(acc_wo_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "1) The accuracy may (no particular pattern seen) decrease after the EM steps. So how many steps to perform is a hyper parameter which has to be tuned carefully. \n",
    "    \n",
    "2) The EM algorithm without the spectral initialization performs as close as the Spectral+EM algorithm. Infact, it sometime outperforms the Spectral+EM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
